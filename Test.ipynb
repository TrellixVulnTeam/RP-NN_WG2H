{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09ebb053",
   "metadata": {},
   "source": [
    "## Loading modules and checking device\n",
    "First, we will load our libraries and check which device we are using, we would prefer using cuda for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffb72022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import unittest\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as tvtf\n",
    "import json\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "seed = 318\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "test = unittest.TestCase()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a202276b",
   "metadata": {},
   "source": [
    "## Importing Datasets\n",
    "Now we will download our different datasets, MNIST, CIFAR-10, CIFAR-100 etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11000a48",
   "metadata": {},
   "source": [
    "*MNIST Download*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daa5d81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 60000 samples\n",
      "Test: 60000 samples\n",
      "MNIST input image size = torch.Size([1, 28, 28])\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 784])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#MNIST DOWNLOAD\n",
    "data_dir = os.path.expanduser('~/.pytorch-datasets')\n",
    "mnist_train = torchvision.datasets.MNIST(root=data_dir, download=True, train=True, transform=tvtf.ToTensor())\n",
    "mnist_test = torchvision.datasets.MNIST(root=data_dir, download=True, train=False, transform=tvtf.ToTensor())\n",
    "#Create BucketIterator for dataset\n",
    "batch_size = 32\n",
    "\n",
    "mdl_train = torch.utils.data.DataLoader(mnist_train, batch_size, shuffle=True)\n",
    "mdl_test = dl_train = torch.utils.data.DataLoader(mnist_test, batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "print(f'Train: {len(mnist_train)} samples')\n",
    "print(f'Test: {len(mnist_train)} samples')\n",
    "\n",
    "x0,y0 = mnist_train[0]\n",
    "in_size = x0.shape\n",
    "num_classes = 10\n",
    "print('MNIST input image size =', in_size)\n",
    "print(y0)\n",
    "def to_image(x):\n",
    "    return (x * torch.ones(3,28,28)).permute(1,2,0)\n",
    "plt.imshow(to_image(x0))\n",
    "\n",
    "X,y = next(iter(mdl_train))\n",
    "X = X.reshape(X.size(0), -1) \n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9dd30e",
   "metadata": {},
   "source": [
    "CIFAR-10 Download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec42913f",
   "metadata": {},
   "source": [
    "## Loading the model\n",
    "We will now load the Linear Classifier model and test it's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff3e5844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearClassifier(\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=32, out_features=10, bias=True)\n",
       "    (5): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from LinearNetwork import LinearClassifier as FCModel\n",
    "mnist_model = FCModel(in_size=784, out_classes=10, activation_type=\"relu\", hidden_dims=[64,32])\n",
    "mnist_model = mnist_model.to(device)\n",
    "\n",
    "mnist_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275b5814",
   "metadata": {},
   "source": [
    "## Performing experiments on the model\n",
    "We will perform training on the original architecture persented at: https://arxiv.org/pdf/1812.09489.pdf on MNIST dataset, and other datasets with more features, we would later train and evaluate on the a new presented architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a571058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will try two main architectures, both with RP layer at first,\n",
    "# it is a Linear layer which does not train and it's weights are\n",
    "# a RP matrix from our implemented module for Random Projection\n",
    "import RandomProjection as RP\n",
    "from Trainer import train_and_eval\n",
    "# we will try varying projected dimension k for the MNIST problem\n",
    "#experiment on different k's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd6878f0-d7de-4cb5-a0be-ee2d21e259b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code block saves results from our results data format, can put in a function\n",
    "def Save_Results(results, rp_type, verbose=False, dataset=\"MNIST\"):\n",
    "    if verbose is True:\n",
    "        for k in results.keys():\n",
    "            print(f\"Projected:{k}: train accuracy:{max(results[k][0])}, test accuracy:{max(results[k][1])}\")\n",
    "\n",
    "    file = open(f\"Results/{dataset}_RP_{rp_type}.txt\",\"w\")\n",
    "    file.write(json.dumps(results))\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e21f5d-7df5-4ef0-b140-9393cc520e8b",
   "metadata": {},
   "source": [
    "**Loading**<br/>We will load the results from files and in that case we would not retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b98e4534-4325-4223-8eba-b1068b6f683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "projections = [\"Gaussian\",\"Achlioptas\", \"Li\", \"CountSketch\", \"SRHT\"] #TODO: add SRHT, needs to add zero paddings to dataset\n",
    "results = {}\n",
    "for proj in projections:\n",
    "    if not os.path.isfile(f\"Results/MNIST_RP_{proj}.txt\"):\n",
    "        continue\n",
    "    file = open(f\"Results/MNIST_RP_{proj}.txt\",\"r+\")\n",
    "    results[proj] = json.loads(file.read())\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b170e55e-7de9-42e7-be1a-145928611838",
   "metadata": {},
   "source": [
    "In any other case we retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f82cf544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training for SRHT\n",
      "projected dimension:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Documents\\Technion\\Tutorials\\Mass Information\\project\\Code\\LinearNetwork.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  W = torch.tensor(W, dtype=torch.float32)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-a2a89f9b5e38>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mloss_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_and_eval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmdl_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmdl_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mproj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Technion\\Tutorials\\Mass Information\\project\\Code\\Trainer.py\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[1;34m(model, train_iter, valid_iter, optimizer, loss_fn, epochs, dir, name, data_name, verbose)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;31m#evaluate over validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mval_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalid_iter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cs236781-hw\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cs236781-hw\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    404\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cs236781-hw\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cs236781-hw\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cs236781-hw\\lib\\site-packages\\torchvision\\datasets\\mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cs236781-hw\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \"\"\"\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\cs236781-hw\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "krange = torch.logspace(1,9,9,base=2) #we test k's from 1 to 784 with multiplicative steps of 2\n",
    "padding = 0\n",
    "for proj in projections:\n",
    "    if proj in results.keys(): #We would not retrain any model alreay trained (if krange list changed we would want to retrain)\n",
    "        continue\n",
    "    results[proj] = {}\n",
    "    print(f\"training for {proj}\")\n",
    "    if proj == \"SRHT\":\n",
    "        padding = 1024-784 #TODO: make this line adaptive for input size\n",
    "    for k in list(krange)[::1]:\n",
    "        mnist_model = FCModel(in_size=784, out_classes=10, activation_type=\"relu\", hidden_dims=[int(k/2)], rp=int(k), rp_type=proj, padding=padding).to(device)\n",
    "        print(f\"projected dimension:{int(k)}\")\n",
    "        #print(f\"Model Architecture:{mnist_model}\")\n",
    "        lr = 1e-3\n",
    "        optimizer = torch.optim.Adam(mnist_model.parameters(), lr=lr)\n",
    "        loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "        res = train_and_eval(mnist_model, mdl_train, mdl_test, optimizer, loss_fn, verbose=False)\n",
    "        results[proj][str(int(k))] = res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085ce2af-bae9-4d23-ac81-ebb528d96e4f",
   "metadata": {},
   "source": [
    "**We will save the results to a txt file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6db3c64-3f18-4c78-a665-a59c0fc68672",
   "metadata": {},
   "outputs": [],
   "source": [
    "for proj in projections:\n",
    "    print(proj)\n",
    "    Save_Results(results[proj], proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8438d7c7",
   "metadata": {},
   "source": [
    "## Graphs\n",
    "## TODO: ADD SRHT !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "Showing the results for different projection dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bace09a-981a-49c1-97a3-8e24630c4df6",
   "metadata": {},
   "source": [
    "For each Random Projection, we will display the best result in each measured parameter (i.e train and test loss and accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40b7ef7-32e6-4fe9-b0e1-9eaa9f614bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = {}\n",
    "for proj in results.keys():\n",
    "    final[proj] = {}\n",
    "    for k in results[proj].keys():\n",
    "        for i in range(4): # 4 different measurements of accuracy/loss\n",
    "            temp = max(results[proj][k][i][:2]) if i <= 1 else min(results[proj][k][i][:2])\n",
    "            if i in final[proj].keys():\n",
    "                final[proj][i].append(temp)\n",
    "            else:\n",
    "                final[proj][i] = [temp]\n",
    "            \n",
    "krange = torch.logspace(1,9,9,base=2) #we test k's from 1 to 784 with multiplicative steps of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d44619-b842-43be-af11-a2fa76bfc4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results, arr, label=\"k\", xscale=\"linear\", xbase=2.0): #results- list of results from different models, each result is a list\n",
    "    colors = [\"red\",\"blue\",\"green\",\"black\",\"purple\"]\n",
    "    fig,_ = plt.subplots(nrows=2,ncols=2,sharex=True, figsize = (18,12))\n",
    "    k = [int(i) for i in arr]\n",
    "    for i, plot_type in enumerate(('train_acc', 'train_loss', 'valid_acc', 'valid_loss')):\n",
    "        ax = fig.axes[i]\n",
    "        if xscale == \"log\":\n",
    "            ax.set_xscale(xscale, base=xbase)\n",
    "        ax.set_yscale(\"linear\")\n",
    "        models_res = results[plot_type]\n",
    "        idx = 0\n",
    "        for model in models_res.keys():\n",
    "            ax.plot(np.array(k), models_res[model], label=model, color=colors[idx])\n",
    "            idx += 1\n",
    "        ax.set_title(plot_type)\n",
    "        ax.set_xlabel(label)\n",
    "        ax.legend()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4540dfe-2fc8-4a4d-988e-692202c7cc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results[\"Li\"][\"32\"][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f643a0f8-444a-4858-a97c-5cd24b43000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_m = {'train_acc':None,'valid_acc':None,'train_loss':None,'valid_loss':None}\n",
    "results_m['train_acc'] = {'Gaussian':final[\"Gaussian\"][0],'Achlioptas':final[\"Achlioptas\"][0],'Li':final[\"Li\"][0], \"Count-Sketch\": final[\"CountSketch\"][0]}\n",
    "results_m['valid_acc'] = {'Gaussian':final[\"Gaussian\"][1],'Achlioptas':final[\"Achlioptas\"][1],'Li':final[\"Li\"][1], \"Count-Sketch\": final[\"CountSketch\"][1]}\n",
    "results_m['train_loss'] = {'Gaussian':final[\"Gaussian\"][2],'Achlioptas':final[\"Achlioptas\"][2],'Li':final[\"Li\"][2], \"Count-Sketch\": final[\"CountSketch\"][2]}\n",
    "results_m['valid_loss'] = {'Gaussian':final[\"Gaussian\"][3],'Achlioptas':final[\"Achlioptas\"][3],'Li':final[\"Li\"][3], \"Count-Sketch\": final[\"CountSketch\"][3]}\n",
    "\n",
    "plot_results(results_m, krange, xscale=\"log\", xbase=2.0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9520d45b-2118-47eb-86c7-7086005196e2",
   "metadata": {},
   "source": [
    "**We will now print the convergance graphs of the model for different projection dimensions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d799a21-d89d-48c6-a060-252b81efbe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(20)\n",
    "for k in [32,64,128,256,512]:\n",
    "    results_m = {'train_acc':None,'valid_acc':None,'train_loss':None,'valid_loss':None}\n",
    "    results_m['train_acc'] = {'Gaussian':results[\"Gaussian\"][\"32\"][0],'Achlioptas':results[\"Achlioptas\"][\"32\"][0],'Li':results[\"Li\"][\"32\"][0], \"Count-Sketch\": results[\"CountSketch\"][\"32\"][0]}\n",
    "    results_m['valid_acc'] = {'Gaussian':results[\"Gaussian\"][\"32\"][1],'Achlioptas':results[\"Achlioptas\"][\"32\"][1],'Li':results[\"Li\"][\"32\"][1], \"Count-Sketch\": results[\"CountSketch\"][\"32\"][1]}\n",
    "    results_m['train_loss'] = {'Gaussian':results[\"Gaussian\"][\"32\"][2],'Achlioptas':results[\"Achlioptas\"][\"32\"][2],'Li':results[\"Li\"][\"32\"][2], \"Count-Sketch\": results[\"CountSketch\"][\"32\"][2]}\n",
    "    results_m['valid_loss'] = {'Gaussian':results[\"Gaussian\"][\"32\"][3],'Achlioptas':results[\"Achlioptas\"][\"32\"][3],'Li':results[\"Li\"][\"32\"][3], \"Count-Sketch\": results[\"CountSketch\"][\"32\"][3]}\n",
    "    plot_results(results_m, epochs, f\"epoch; k={k}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0a3086-4637-41bd-93af-830412c976d2",
   "metadata": {},
   "source": [
    "In the first graph (best accuracy to projected dimension) we can see that the difference in performance between the different methods is not major.<br/>\n",
    "In the next graphs we can see clearly that for MNIST dataset, Achlioptas Random Projection performs better for a large enough amount of epochs, for a smaller number of epochs, Li's method performs better on the validation set, for a really small number of epochs we can see that the Gaussian Random Projection performs better than Achlioptas as well, Count-Sketch shows the least accuracy of all for this dataset, possibly because of the small input size (that would be tested later on a larger dataset).\n",
    "A possible explanation of the results is that Li's Random Projection is similiar to Achlioptas but more sparse, this sparsity preserves less information but the information preserved is easier to learn from after very few epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33adc3e",
   "metadata": {},
   "source": [
    "Downloading CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f9458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CIFAR-10 DOWNLOAD\n",
    "data_dir = os.path.expanduser('~/.pytorch-datasets')\n",
    "cifar10_train = torchvision.datasets.CIFAR10(root=data_dir, download=True, train=True, transform=tvtf.ToTensor())\n",
    "cifar10_test = torchvision.datasets.CIFAR10(root=data_dir, download=True, train=False, transform=tvtf.ToTensor())\n",
    "\n",
    "print(f'Train: {len(cifar10_train)} samples')\n",
    "print(f'Test: {len(cifar10_test)} samples')\n",
    "\n",
    "x0,_ = cifar10_train[0]\n",
    "in_size = x0.shape\n",
    "num_classes = 10\n",
    "print('CIFAR-10 input image size =', in_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f24b3f",
   "metadata": {},
   "source": [
    "## Download datasets (To be added)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608718e7",
   "metadata": {},
   "source": [
    "<h4>Importing our experiments</h4>\n",
    "Now, we will load our experiments module for testing the different architectures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb488b14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
