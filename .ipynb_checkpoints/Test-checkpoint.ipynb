{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2976f3c1",
   "metadata": {},
   "source": [
    "## Loading modules and checking device\n",
    "First, we will load our libraries and check which device we are using, we would prefer using cuda for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62147685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import unittest\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as tvtf\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "seed = 318\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "test = unittest.TestCase()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5855fad1",
   "metadata": {},
   "source": [
    "## Importing Datasets\n",
    "Now we will download our different datasets, MNIST, CIFAR-10, CIFAR-100 etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c781e52d",
   "metadata": {},
   "source": [
    "MNIST Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbadfc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 60000 samples\n",
      "Test: 60000 samples\n",
      "MNIST input image size = torch.Size([1, 28, 28])\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 784])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#MNIST DOWNLOAD\n",
    "data_dir = os.path.expanduser('~/.pytorch-datasets')\n",
    "mnist_train = torchvision.datasets.MNIST(root=data_dir, download=True, train=True, transform=tvtf.ToTensor())\n",
    "mnist_test = torchvision.datasets.MNIST(root=data_dir, download=True, train=False, transform=tvtf.ToTensor())\n",
    "#Create BucketIterator for dataset\n",
    "batch_size = 32\n",
    "\n",
    "mdl_train = torch.utils.data.DataLoader(mnist_train, batch_size, shuffle=True)\n",
    "mdl_test = dl_train = torch.utils.data.DataLoader(mnist_test, batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "print(f'Train: {len(mnist_train)} samples')\n",
    "print(f'Test: {len(mnist_train)} samples')\n",
    "\n",
    "x0,y0 = mnist_train[0]\n",
    "in_size = x0.shape\n",
    "num_classes = 10\n",
    "print('MNIST input image size =', in_size)\n",
    "print(y0)\n",
    "def to_image(x):\n",
    "    return (x * torch.ones(3,28,28)).permute(1,2,0)\n",
    "plt.imshow(to_image(x0))\n",
    "\n",
    "X,y = next(iter(mdl_train))\n",
    "X = X.reshape(X.size(0), -1) \n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ffa518",
   "metadata": {},
   "source": [
    "CIFAR-10 Download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520537d0",
   "metadata": {},
   "source": [
    "## Loading the model\n",
    "We will now load the Linear Classifier model and test it's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c8ca657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearClassifier(\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=32, out_features=10, bias=True)\n",
       "    (5): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from LinearNetwork import LinearClassifier as FCModel\n",
    "mnist_model = FCModel(in_size=784, out_classes=10, activation_type=\"relu\", hidden_dims=[64,32])\n",
    "mnist_model = mnist_model.to(device)\n",
    "\n",
    "mnist_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1df89e",
   "metadata": {},
   "source": [
    "**Performing experiments on the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7369993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0,train loss=1.486,train accuracy=0.976, validation accuracy=0.968, elapsed=10.4 sec\n",
      "*** Saved checkpoint Models/FC_RNN.pt at epoch 1\n",
      "Epoch #1,train loss=1.485,train accuracy=0.977, validation accuracy=0.967, elapsed=10.3 sec\n",
      "Epoch #2,train loss=1.483,train accuracy=0.979, validation accuracy=0.968, elapsed=10.4 sec\n",
      "Epoch #3,train loss=1.482,train accuracy=0.980, validation accuracy=0.970, elapsed=10.6 sec\n",
      "*** Saved checkpoint Models/FC_RNN.pt at epoch 4\n",
      "Epoch #4,train loss=1.481,train accuracy=0.981, validation accuracy=0.970, elapsed=10.6 sec\n",
      "Epoch #5,train loss=1.480,train accuracy=0.982, validation accuracy=0.967, elapsed=10.6 sec\n",
      "Epoch #6,train loss=1.479,train accuracy=0.982, validation accuracy=0.970, elapsed=10.6 sec\n",
      "Epoch #7,train loss=1.479,train accuracy=0.983, validation accuracy=0.971, elapsed=10.5 sec\n",
      "*** Saved checkpoint Models/FC_RNN.pt at epoch 8\n",
      "Epoch #8,train loss=1.478,train accuracy=0.983, validation accuracy=0.972, elapsed=10.6 sec\n",
      "*** Saved checkpoint Models/FC_RNN.pt at epoch 9\n",
      "Epoch #9,train loss=1.477,train accuracy=0.984, validation accuracy=0.971, elapsed=10.6 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36m_getencoder\u001b[0;34m(mode, encoder_name, args, extra)\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mENCODERS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoder_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'raw'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-ddedd1392868>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdl_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdl_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/deepAccel/rp-nn/Trainer.py\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(model, train_iter, valid_iter, optimizer, loss_fn, epochs, dir, name, data_name, verbose)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.8/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \"\"\"\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteStorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mtobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;31m# unpack data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_getencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m         \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36m_getencoder\u001b[0;34m(mode, encoder_name, args, extra)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mENCODERS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoder_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# We will try two main architectures, both with RP layer at first,\n",
    "# it is a Linear layer which does not train and it's weights are\n",
    "# a RP matrix from our implemented module for Random Projection\n",
    "import RandomProjection as RP\n",
    "from Trainer import train_and_eval\n",
    "# we will try varying projected dimension k for the MNIST problem\n",
    "#experiment on different k's\n",
    "\n",
    "\n",
    "#experiment on different activations and different models (hidden_dims)\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam(mnist_model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "res = train_and_eval(mnist_model, mdl_train, mdl_test, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b3297a8-5341-4297-b3c9-4d2bfc3c97de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture:LinearClassifier(\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1, out_features=10, bias=True)\n",
      "    (3): Softmax(dim=1)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-04d4867d3d9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmnist_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFCModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model Architecture:{mnist_model}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdl_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdl_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/deepAccel/rp-nn/Trainer.py\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(model, train_iter, valid_iter, optimizer, loss_fn, epochs, dir, name, data_name, verbose)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_log_proba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m# Weight updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs236781-hw/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "krange = torch.logspace(1,10,10,base=2) #we test k's from 1 to 784 with multiplicative steps of 2\n",
    "results = []\n",
    "for k in krange:\n",
    "    mnist_model = FCModel(in_size=784, out_classes=10, activation_type=\"relu\", hidden_dims=[int(k/2)]).to(device)\n",
    "    print(f\"Model Architecture:{mnist_model}\")\n",
    "    res = train_and_eval(mnist_model, mdl_train, mdl_test, optimizer, loss_fn, verbose=False)\n",
    "    print(res)\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edca9238",
   "metadata": {},
   "source": [
    "Downloading CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e555a802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train: 50000 samples\n",
      "Test: 10000 samples\n",
      "CIFAR-10 input image size = torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "#CIFAR-10 DOWNLOAD\n",
    "data_dir = os.path.expanduser('~/.pytorch-datasets')\n",
    "cifar10_train = torchvision.datasets.CIFAR10(root=data_dir, download=True, train=True, transform=tvtf.ToTensor())\n",
    "cifar10_test = torchvision.datasets.CIFAR10(root=data_dir, download=True, train=False, transform=tvtf.ToTensor())\n",
    "\n",
    "print(f'Train: {len(cifar10_train)} samples')\n",
    "print(f'Test: {len(cifar10_test)} samples')\n",
    "\n",
    "x0,_ = cifar10_train[0]\n",
    "in_size = x0.shape\n",
    "num_classes = 10\n",
    "print('CIFAR-10 input image size =', in_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1929e2e",
   "metadata": {},
   "source": [
    "## Download datasets (To be added)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ae1f7a",
   "metadata": {},
   "source": [
    "<h4>Importing our experiments</h4>\n",
    "Now, we will load our experiments module for testing the different architectures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc405fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
